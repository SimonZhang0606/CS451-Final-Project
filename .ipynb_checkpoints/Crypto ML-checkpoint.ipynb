{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5d65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I used this link to make a boiler plate for the spark code, we may not need all of it, but it is a good staritng point\n",
    "\n",
    "#https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91271163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('ml-blockchain').getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd29b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#script to load in the data\n",
    "\n",
    "dataframe = \"data/data\"\n",
    "\n",
    "print(dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#script to transform the json file into pandas \n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read(dataframe) # or something \n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need seperate script \n",
    "df = spark.read.csv(dataframe, header = True, inferSchema = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do a little EDA \n",
    "\n",
    "\n",
    "#fluctuation and distribution of the price \n",
    "\n",
    "\n",
    "#quantity of transactions, and the distribution of the transactions \n",
    "\n",
    "\n",
    "#time periods where the transactions spikes etc. \n",
    "\n",
    "#summary statistics\n",
    "\n",
    "numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\n",
    "df.select(numeric_features).describe().toPandas().transpose()\n",
    "\n",
    "#correlation\n",
    "numeric_data = df.select(numeric_features).toPandas()\n",
    "axs = pd.scatter_matrix(numeric_data, figsize=(8, 8));\n",
    "n = len(numeric_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())\n",
    "\n",
    "    \n",
    "#remove variables mannually if the variables have low correlation\n",
    "\n",
    "df = df.select(#all needed ones)\n",
    "cols = df.columns\n",
    "df.printSchema()\n",
    "    \n",
    "    \n",
    "\n",
    "#one idea is to take the middle area of the transacitons (going up and down only a little, and removing those columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cff00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the data, if needed  i.e. normalize, fill in missing fields etc. \n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "\n",
    "#categorial \n",
    "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "stages = []\n",
    "\n",
    "#one hot encoding if needed \n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "    \n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "numericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025db169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the data using pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df = pipelineModel.transform(df)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "df = df.select(selectedCols)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb66a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "train, test = df.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f542ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66412628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing results \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "beta = np.sort(lrModel.coefficients)\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e92928",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b565d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86d7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
